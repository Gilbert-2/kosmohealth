<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Processing Test</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 20px;
      background-color: #f5f5f5;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
      background-color: white;
      padding: 20px;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }
    h1 {
      color: #333;
      text-align: center;
    }
    .video-container {
      position: relative;
      width: 100%;
      margin: 20px 0;
    }
    video {
      width: 100%;
      border-radius: 8px;
      background-color: #000;
    }
    .controls {
      margin-top: 20px;
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
    }
    .control-group {
      flex: 1;
      min-width: 200px;
      background-color: #f9f9f9;
      padding: 15px;
      border-radius: 8px;
    }
    .switch {
      display: flex;
      align-items: center;
      margin-bottom: 10px;
    }
    .switch label {
      margin-left: 10px;
    }
    .status {
      margin-top: 20px;
      padding: 10px;
      border-radius: 4px;
      background-color: #e9f5ff;
    }
    .emotion-display {
      position: absolute;
      top: 10px;
      right: 10px;
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 5px 10px;
      border-radius: 4px;
      font-weight: bold;
      display: none;
    }
    .notification {
      position: fixed;
      top: 20px;
      left: 50%;
      transform: translateX(-50%);
      background-color: rgba(255, 193, 7, 0.9);
      color: black;
      padding: 10px 15px;
      border-radius: 4px;
      font-weight: bold;
      display: none;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
      z-index: 9999;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Face Processing Test</h1>
    
    <div class="video-container">
      <video id="localVideo" autoplay playsinline></video>
      <canvas id="faceProcessingCanvas"></canvas>
      <div id="emotionDisplay" class="emotion-display"></div>
    </div>
    
    <div class="controls">
      <div class="control-group">
        <h3>Face Processing Controls</h3>
        <div class="switch">
          <input type="checkbox" id="blurFaceToggle">
          <label for="blurFaceToggle">Blur Face</label>
        </div>
        <div class="switch">
          <input type="checkbox" id="emotionDetectionToggle">
          <label for="emotionDetectionToggle">Emotion Detection</label>
        </div>
        <div class="switch">
          <input type="checkbox" id="autoBlurToggle" checked>
          <label for="autoBlurToggle">Auto-Blur on Discomfort</label>
        </div>
      </div>
      
      <div class="control-group">
        <h3>Camera Controls</h3>
        <button id="startCameraBtn">Start Camera</button>
        <button id="stopCameraBtn" disabled>Stop Camera</button>
      </div>
    </div>
    
    <div class="status" id="status">
      Status: Waiting for camera...
    </div>
  </div>
  
  <div id="notification" class="notification">Discomfort detected</div>

  <!-- Load face-api.js -->
  <script src="/js/face-api.min.js"></script>
  
  <script>
    // DOM Elements
    const localVideo = document.getElementById('localVideo');
    const canvas = document.getElementById('faceProcessingCanvas');
    const emotionDisplay = document.getElementById('emotionDisplay');
    const notification = document.getElementById('notification');
    const status = document.getElementById('status');
    const startCameraBtn = document.getElementById('startCameraBtn');
    const stopCameraBtn = document.getElementById('stopCameraBtn');
    const blurFaceToggle = document.getElementById('blurFaceToggle');
    const emotionDetectionToggle = document.getElementById('emotionDetectionToggle');
    const autoBlurToggle = document.getElementById('autoBlurToggle');
    
    // Configuration
    const config = {
      enabled: true,
      showControls: true,
      showCanvas: true,
      showEmotionInfo: true,
      autoBlurOnDiscomfort: true,
      processingFps: 3,
      discomfortEmotions: ['angry', 'fearful', 'disgusted', 'sad'],
      emotionThreshold: 0.7
    };
    
    // State
    let state = {
      modelsLoaded: false,
      isProcessing: false,
      blurEnabled: false,
      emotionDetectionEnabled: false,
      lastEmotions: null,
      processingInterval: null,
      discomfortDetected: false,
      notificationTimeout: null
    };
    
    // Initialize
    async function init() {
      try {
        // Set up canvas
        canvas.width = localVideo.videoWidth || 640;
        canvas.height = localVideo.videoHeight || 480;
        canvas.style.position = 'absolute';
        canvas.style.top = '0';
        canvas.style.left = '0';
        canvas.style.width = '100%';
        canvas.style.height = '100%';
        canvas.style.display = 'none';
        
        // Load face-api.js models
        await loadModels();
        
        // Set up event listeners
        setupEventListeners();
        
        status.textContent = 'Status: Ready. Click "Start Camera" to begin.';
      } catch (error) {
        console.error('Error initializing:', error);
        status.textContent = 'Status: Error initializing. See console for details.';
      }
    }
    
    // Load face-api.js models
    async function loadModels() {
      try {
        status.textContent = 'Status: Loading face-api.js models...';
        
        // Set the models path
        const MODEL_URL = '/js/face-api-models';
        
        // Load models
        await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
        await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
        await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);
        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
        
        state.modelsLoaded = true;
        status.textContent = 'Status: Models loaded successfully.';
        return true;
      } catch (error) {
        console.error('Error loading face-api models:', error);
        status.textContent = 'Status: Error loading models. See console for details.';
        return false;
      }
    }
    
    // Set up event listeners
    function setupEventListeners() {
      startCameraBtn.addEventListener('click', startCamera);
      stopCameraBtn.addEventListener('click', stopCamera);
      blurFaceToggle.addEventListener('change', toggleBlur);
      emotionDetectionToggle.addEventListener('change', toggleEmotionDetection);
      autoBlurToggle.addEventListener('change', toggleAutoBlur);
    }
    
    // Start camera
    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ 
          video: true, 
          audio: false 
        });
        
        localVideo.srcObject = stream;
        
        localVideo.onloadedmetadata = () => {
          canvas.width = localVideo.videoWidth;
          canvas.height = localVideo.videoHeight;
          startProcessing();
          
          startCameraBtn.disabled = true;
          stopCameraBtn.disabled = false;
          status.textContent = 'Status: Camera started. Processing video...';
        };
      } catch (error) {
        console.error('Error starting camera:', error);
        status.textContent = 'Status: Error starting camera. See console for details.';
      }
    }
    
    // Stop camera
    function stopCamera() {
      stopProcessing();
      
      if (localVideo.srcObject) {
        localVideo.srcObject.getTracks().forEach(track => track.stop());
        localVideo.srcObject = null;
      }
      
      startCameraBtn.disabled = false;
      stopCameraBtn.disabled = true;
      status.textContent = 'Status: Camera stopped.';
    }
    
    // Toggle blur
    function toggleBlur() {
      state.blurEnabled = blurFaceToggle.checked;
      canvas.style.display = state.blurEnabled ? 'block' : 'none';
    }
    
    // Toggle emotion detection
    function toggleEmotionDetection() {
      state.emotionDetectionEnabled = emotionDetectionToggle.checked;
      emotionDisplay.style.display = state.emotionDetectionEnabled ? 'block' : 'none';
    }
    
    // Toggle auto blur
    function toggleAutoBlur() {
      config.autoBlurOnDiscomfort = autoBlurToggle.checked;
    }
    
    // Start processing
    function startProcessing() {
      if (!state.modelsLoaded) {
        console.error('Models not loaded. Call loadModels() first.');
        return;
      }
      
      const intervalMs = 1000 / config.processingFps;
      
      state.processingInterval = setInterval(() => {
        processFrame();
      }, intervalMs);
    }
    
    // Stop processing
    function stopProcessing() {
      if (state.processingInterval) {
        clearInterval(state.processingInterval);
        state.processingInterval = null;
      }
    }
    
    // Process video frame
    async function processFrame() {
      if (!state.modelsLoaded || state.isProcessing || !localVideo || 
          localVideo.paused || localVideo.ended) {
        return;
      }

      state.isProcessing = true;

      try {
        // Detect all faces with expressions
        const detections = await faceapi.detectAllFaces(
          localVideo, 
          new faceapi.TinyFaceDetectorOptions()
        )
        .withFaceLandmarks()
        .withFaceExpressions();

        // Process emotions if enabled
        if (state.emotionDetectionEnabled && detections.length > 0) {
          processEmotions(detections);
        }

        // Draw results on canvas if blur is enabled
        if (state.blurEnabled && detections.length > 0) {
          const dims = faceapi.matchDimensions(canvas, localVideo, true);
          const resizedDetections = faceapi.resizeResults(detections, dims);
          
          // Clear canvas
          const ctx = canvas.getContext('2d');
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          
          // Apply blur
          applyFaceBlur(ctx, resizedDetections);
        }
      } catch (error) {
        console.error('Error processing video frame:', error);
      } finally {
        state.isProcessing = false;
      }
    }
    
    // Process emotions from detections
    function processEmotions(detections) {
      if (!detections || detections.length === 0) return;
      
      // Get the primary face (usually the largest or most centered)
      const primaryFace = detections[0];
      const expressions = primaryFace.expressions;
      
      // Find the dominant emotion
      let dominantEmotion = null;
      let highestScore = 0;
      
      for (const [emotion, score] of Object.entries(expressions)) {
        if (score > highestScore) {
          highestScore = score;
          dominantEmotion = emotion;
        }
      }
      
      // Only report if we have a confident detection
      if (highestScore > config.emotionThreshold) {
        state.lastEmotions = {
          dominant: dominantEmotion,
          score: highestScore,
          all: expressions
        };
        
        // Update emotion display
        updateEmotionDisplay(dominantEmotion, highestScore);
        
        // Check if the emotion indicates discomfort
        if (config.discomfortEmotions.includes(dominantEmotion)) {
          handleDiscomfort(dominantEmotion, highestScore);
        }
      }
    }
    
    // Update emotion display
    function updateEmotionDisplay(emotion, score) {
      emotionDisplay.textContent = emotion.charAt(0).toUpperCase() + emotion.slice(1);
      emotionDisplay.style.display = 'block';
      
      // Set color based on emotion
      const emotionColors = {
        'happy': 'rgba(76, 175, 80, 0.9)',
        'sad': 'rgba(33, 150, 243, 0.9)',
        'angry': 'rgba(244, 67, 54, 0.9)',
        'fearful': 'rgba(156, 39, 176, 0.9)',
        'disgusted': 'rgba(121, 85, 72, 0.9)',
        'surprised': 'rgba(255, 193, 7, 0.9)',
        'neutral': 'rgba(158, 158, 158, 0.9)'
      };
      
      emotionDisplay.style.backgroundColor = emotionColors[emotion] || 'rgba(0, 0, 0, 0.7)';
    }
    
    // Handle discomfort detection
    function handleDiscomfort(emotion, score) {
      state.discomfortDetected = true;
      
      // Auto-enable blur if configured
      if (config.autoBlurOnDiscomfort && !state.blurEnabled) {
        state.blurEnabled = true;
        blurFaceToggle.checked = true;
        canvas.style.display = 'block';
      }
      
      // Show notification
      notification.style.display = 'block';
      
      // Hide after 5 seconds
      if (state.notificationTimeout) {
        clearTimeout(state.notificationTimeout);
      }
      
      state.notificationTimeout = setTimeout(() => {
        notification.style.display = 'none';
      }, 5000);
    }
    
    // Apply blur effect to faces
    function applyFaceBlur(ctx, detections) {
      if (!detections || detections.length === 0) return;
      
      // Draw the video frame first
      ctx.drawImage(localVideo, 0, 0, canvas.width, canvas.height);
      
      detections.forEach(detection => {
        const { box } = detection.detection;
        
        // Save the current state
        ctx.save();
        
        // Create a clipping region for the face
        ctx.beginPath();
        ctx.rect(box.x, box.y, box.width, box.height);
        ctx.clip();
        
        // Apply a blur filter
        ctx.filter = 'blur(15px)';
        
        // Redraw the video frame in the clipped region with blur
        ctx.drawImage(
          localVideo, 
          box.x, box.y, box.width, box.height, 
          box.x, box.y, box.width, box.height
        );
        
        // Restore the context
        ctx.restore();
      });
    }
    
    // Initialize when the page is loaded
    document.addEventListener('DOMContentLoaded', init);
  </script>
</body>
</html>
